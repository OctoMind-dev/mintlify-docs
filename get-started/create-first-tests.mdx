---
title: Create first tests
description: "Generate your first tests and run them. Find bugs in your app before your users do."
icon: "flag-checkered"
---

## Try Octomind for free! Sign up to Octomind for a demo account

### 1. Sign up with and email

Give us your work email address to create and account in the Octomind app.

<Frame caption="Signing up with email,09/2025">
  <img
    src="/images/get-started/signup-email.png"
    alt="Signing up with email"
    width="400"
  />
</Frame>

<div class="mt-8" />

<Frame caption="Email submitted,09/2025">
  <img
    src="/images/get-started/signup-submitted.png"
    alt="Email submitted, octomind app screenshot"
    width="400"
  />
</Frame>

### 2. Confirm email and set a password

We've sent a confirmation email. After confirming, you'll be asked to create a password for the app login.

### 3. Open the Octomind app for the first time

After being redirected back to [app.octomind.dev](https://app.octomind.dev/), you will land on a ** demo project overview** page.

We have created a demo instance on the Rocket.Chat app - an open-source, self-hostable communication platform for organizations functioning as a secure alternative to platforms like Slack, allowing for real-time messaging, audio/video calls, file sharing, and integrations for teams and customers across web, desktop, and mobile devices.

<Frame caption="Your own Octomind demo instance of a Rocket.Chat app test suite, 09/2025">
  <img
    src="/images/get-started/demo-welcome.png"
    alt="Your own Octomind demo instance of a Rocket.Chat app test suite"
  />
</Frame>

After a filling out a short survey, you'll be able to explore the demo instance we created for you.

<Frame caption="Entry survey, 09/2025">
  <img src="/images/get-started/entry-survey.png" alt="Entry survey" />
</Frame>

### 4. Look into the first tests we created and check the test report

We created example tests for you, using prompts and our agent. Automatically, we have launched a test run of all active cases.

Explore the [test cases](/manage-tests/test-cases) and when the test run is finished, check the [test report](/manage-tests/test-reports) for test run details of tests that passed or failed.

<Frame caption="Example set of test cases in the demo instance, 09/2025">
  <img
    src="/images/get-started/demo-test-cases.png"
    alt="Example set of test cases in the demo instance"
  />
</Frame>

<div class="mt-8" />

<Frame caption="Report of a test run in the demo instance with a failed test, 09/2025">
  <img
    src="/images/get-started/demo-report-failure.png"
    alt="First report of a test run in the demo instance"
  />
</Frame>

### 5. Create new tests

Now, it's your turn to create some test cases. Click on the `get started` button. A selection of `recommended tests` will appear.

<Frame caption="Recommended tests section, 07/2025">
  <img
    src="/images/get-started/recommended-tests.png"
    alt="Recommended tests section"
  />
</Frame>

Our AI Agent has gone through the website, checked what relevant test cases you might want to create first. Select one or refresh to get new recommendations.

Click `next`.

#### For all new projects

**Cookie banner** and a **required login functionality** will be the first ones to be identified - their functionality is usually a prerequisites for all the other tests. They will be added as dependency for other tests.

If the AI Agent finds a required login, it will ask you for test credentials. It will use them to generate and run a **login test**. It will be added as dependency for other tests. So will be the **cookie banner test** if one is found.

<Frame caption="Required login test identified and recommended, 07/2025">
  <img
    src="/images/get-started/login-test-recommended.png"
    alt="Required login test identified and recommended"
  />
</Frame>

<Warning>
These credentials are only usable for username/password logins not for social logins. Login using your Google, Facebook, etc. account will not work.

</Warning>

You can create a login test later. Learn how to create a [pre-prompted login test](/build-tests/prompt-agent#pre-prompted-login-test).

### 6. Refine the prompt

We will generate test cases you selected. But first, we need you to review the prompt we prepared. You can instruct the Agent in natural language to change the prompt here, or refine the prompt manually in the next step.

<Frame caption="Refine prompt example, 07/2025">
  <img
    src="/images/get-started/refine-prompt.png"
    alt="Refine prompt example"
  />
</Frame>

Click `generate` to add details, rename the test or refine the prompt. When everything is all set, click `finalize`.

<Frame caption="Finalize test case, 07/2025">
  <img src="/images/get-started/finalize-test.png" alt="Finalize test case" />
</Frame>

Watch a short video where we showcase these steps on a sandbox app.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/KVURtw5tG58?si=3FKHcniremkbFHuk"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
></iframe>

### 7. We generate the test case

We started generating your test case.

<Frame caption="Test is being generated, 07/2025">
  <img
    src="/images/get-started/generation-ongoing.png"
    alt="test case in the process of being generated"
  />
</Frame>

If you notice the Agent went off track, you can take over the generation process, record your specific few step manually and handover back to the Agent.

Watch how:

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/dIKaGITE08E?si=RGH-ZL8EcF3Ddb2T"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
></iframe>

It's possible that the Agent fails to generate a test ([here are some reasons why](/guides/agent-fails)) or needs help with something - the test is highlighted in **yellow**. This is a [step by step guide on how you'll help the Agent by nudging it in the right direction](/guides/help-agent-succeed).

After being generated successfully, in the right panel, proceed to **validation** to run it and check whether it's working properly.

<Frame caption="validate a generated test case, 07/2025">
  <img
    src="/images/get-started/validate.png"
    alt="Validate a generated test case"
    width="300"
  />
</Frame>

Create more tests using our [recommendations](/build-tests/recommended-tests), [spawn existing tests](/build-tests/recommended-tests#spawning-from-existing-test-cases) or [prompt new tests from scratch](/build-tests/prompt-agent).

### 8. Batch test generation

To speed up the process of creating a test suite, you can also use our **batch generation process**. It is a more guided auto-discovery process - the Agent automatically discovers test cases in the area you gave it via natural language.

Start creating a new test case. After selecting the starting point, chose the `batch create` tab. Instruct the Agent what kind of tests to discover.

<Frame caption="Example of a batch create instruction, 09/2025">
  <img
    src="/images/get-started/batch-create.png"
    alt="Example of a batch create instruction"
  />
</Frame>

The Agent will start exploring relevant test cases, generating them and validating. This might take a while.

<Frame caption="Agent generating a batch of tests as instructed, 09/2025">
  <img
    src="/images/get-started/batch-generation.png"
    alt="Agent generating a batch of tests as instructed"
  />
</Frame>

## Run tests to check the demo app for bugs

Once you've built your test suite, you can [execute your active test cases](/get-started/execute-tests) to check your app for bugs. Every test execution will produce a [test report](/manage-tests/test-reports) with details of the run and debugging information in case of a failed test.

## Evaluate your test results

Inside each test report, you can find the test results for the executed test cases:

- a green test result indicates a successful test run, meaning that your site passed the test described in the test case
- a red test result indicates a test failure, meaning we could not successfully run the test case steps. These are the [possible reasons for failed test](/maintain-tests/why-tests-fail).

Click on it to see in which step is broken and [diagnose the reason for the failure](/maintain-tests/inspect-broken-tests). If it's fixable, you can [debug it](/maintain-tests/inspect-broken-tests#what-to-do-next%3F-how-to-i-fix-the-test%3F).

Find out more about [test reports](/manage-tests/test-reports).

<Frame caption="Example of a Octomind test report, 05/2025">
  <img
    src="/images/manage/test-report.png"
    alt="Example of a Octomind test report"
  />
</Frame>
