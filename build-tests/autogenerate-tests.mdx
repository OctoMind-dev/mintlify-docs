---
title: Autogenerate tests
description: "Use our AI agent to suggest relevant test cases and spawn existing ones to increase your coverage."
icon: "wand-sparkles"
---

## Test autogeneration at sign-up

When you sign up to Octomind, our AI Agent is going on a discovery journey of your site. It screen it and tries to come up with a few relevant tests to start with automatically.

### Cookie banner and login test

First, it's checking if there is a **cookie banner** and a **required login** that need to be tested. "Required login" is relevant for SaaS apps that unfold most of their functionality to logged in users.

<Frame caption="Cookie test is being genetated at sign-up, 05/2025">
  <img
    src="/images/build/cookie-test-generation-at-signup.png"
    alt="Cookie test is being genetated at sign-up to Octomind app"
  />
</Frame>

If the AI Agent finds a required login, it will ask you for test credentials. It will use them to generate and run a **login test**. It will be added as dependency for other tests. So will be the **cookie banner test** if one is found.

We can handle [2-factor authentification (2FA)](/advanced/2fa.mdx) and [captcha](/runtime/custom-headers.mdx) in login flows.

<Frame caption="AI agent asks for test credentials to autogenerate and run a login test, 05/2025">
  <img
    src="/images/build/autogeneration-login-found.png"
    alt="AI agent asks for test credentials to autogenerate and run a login test"
  />
</Frame>

<div class="mt-8" />

<Warning>
  These credentials are only usable for username/password logins not for social
  logins. Login using your Google, Facebook, etc. account will not work.
</Warning>

After these tests are autogenerated, we will validate them to see whether they work. If passed, they will be set as **active** meaning they will run when you [execute your test suite](/get-started/execute-test-cases).

<Frame caption="Autogenerated and validated test set as active, 05/2025">
  <img
    src="/images/build/autogenerated-active-test.png"
    alt="Autogenerated and validated test set as active"
  />
</Frame>

### First batch of tests is generated automatically

Here comes the really cool part. Once we finished searching for a potential **login** and **cookie banner** test, we start generating 3 test cases for you automatically. You can follow the generation progress in the Agent progress bar:

<Frame caption="Agent progress bar showing ongoing AI tasks, 10/2024">
  <img
    src="/images/setup/setup-6-stack.png"
    alt="Agent progress bar showing ongoing AI tasks"
  />
</Frame>

It's possible that the Agent fails to generate a test ([here are some reasons why](/guides/agent-fails)) or needs help with something - the test is highlighed in **yellow**. This is a [step by step guide on how you'll help the Agent by nudging it in the right direction](/guides/help-agent-succeed).

## Spawing existing test cases

Similar to the initial **AI test generation** during project setup you can use the `generate more from here` feature and `create a new test from here` to expand your test case suite.

Go into the test case view and click on the `plus` icon. Two options open:

<Frame caption="Have the AI agent generate more tests, 12/2024">
  <img
    src="/images/build/generate-more.png"
    alt="have AI generate more tests"
  />
</Frame>

With the first - `generate more from here` - our AI Agent will discover and auto-generate up to 3 new tests "following" the test case you launched it from. The original test will be added as a **dependency** automatically.

The second option - `create new test from here` - allows you to prompt the Agent with a specific test flow instruction to create a test "following" the test you launched it from.

<Frame caption="Prompt the Agent to generate a specific test case following a spawned test, 12/2024">
  <img
    src="/images/build/create-more-from-here-prompt.png"
    alt="Prompt the Agent to generate a specific test case following a spawned test"
  />
</Frame>

You can follow the progress of these newly created tests in the **AI Agent progress bar**.

<Frame caption="AI agent informs what it does when spawning a test case, 10/2024">
  <img
    src="/images/build/generating-more-agent-running.png"
    alt="AI agent informs what it does when spawning a test case"
  />
</Frame>

At the end of the agent run, the Agent should have autogenerated test steps and validated them for each test case. For those that the validation succeeded, it turned the test **ON** - [into active mode](/get-started/execute-test-cases).

Help the Agent when it couldn't quite nail the autogeneration process - **yellow alert** highlights a failed step generation. See how you [help the Agent to generate a test case successfully](/guides/help-agent-succeed).

<Frame caption="Autogenerated test with yellow alert - a test step failed to generate, 10/2024">
  <img
    src="/images/build/generated-test-yellow-alert.png"
    alt="Autogenerated tests with yellow alert - a test step failed to generate"
  />
</Frame>
